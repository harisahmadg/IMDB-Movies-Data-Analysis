{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "import math\n",
    "from statistics import *\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)\n",
    "We can determine the most important features of this dataset using PCA. We can trade accuracy for \n",
    "simplicity as analyzing data with fewer dimensions is much easier and faster for machine learning\n",
    "algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets do a PCA analysis on a merged dataset combining title.ratings and title.basics as it contains 5 **useful** features out of 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "title.ratings.csv preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titleRatingsDf = pd.read_csv('dataset/originalDataset/title.ratings.csv', sep='\\t', low_memory=False)\n",
    "titleRatingsDf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "titlet.basics.csv preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titleBasicsDf = pd.read_csv('dataset/originalDataset/title.basics.csv', sep='\\t', low_memory=False)\n",
    "titleBasicsDf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge and Remove Null Entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_merged = pd.merge(titleRatingsDf, titleBasicsDf, on=[\"tconst\"])\n",
    "inner_merged = inner_merged[['averageRating', 'numVotes', 'isAdult', 'startYear', 'runtimeMinutes']]\n",
    "\n",
    "# Remove Null Entries\n",
    "def removeNa(df):\n",
    "    to_nan = {\n",
    "        \"\": np.nan,\n",
    "        \" \": np.nan,\n",
    "        '\\\\N': np.nan\n",
    "    }\n",
    "    df.replace(to_nan, inplace=True)\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "main_df = removeNa(inner_merged)\n",
    "main_df = main_df[['averageRating', 'numVotes', 'startYear', 'runtimeMinutes']] # remove isAdult, binary not good for model\n",
    "# main_df.to_csv('Results/ModelData.csv', index=False, header=['averageRating', 'numVotes', 'startYear', 'runtimeMinutes'])\n",
    "main_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess, scale and standardize the data before doing PCA on it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data = preprocessing.scale(main_df)\n",
    "pca = PCA()\n",
    "pca.fit(scaled_data)\n",
    "pca_data = pca.transform(scaled_data)\n",
    "\n",
    "per_var = np.round(pca.explained_variance_ratio_* 100, decimals=1)\n",
    "labels = ['PC' + str(x) for x in range(1, len(per_var)+1)]\n",
    "\n",
    "# Viusalize scree plot\n",
    "plt.bar(x=range(1, len(per_var)+1), height=per_var, tick_label=labels, edgecolor='black')\n",
    "plt.xlabel(\"Princple Component\")\n",
    "plt.ylabel(\"Percentage Explained Variance\")\n",
    "plt.title(\"Scree Plot\")\n",
    "# plt.savefig('Results/Scree Plot.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the PCA Graph by using the 2 most significant PCA's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df = pd.DataFrame(pca_data, index=pca_data.T[0], columns=labels)\n",
    "plt.scatter(pca_df.PC1, pca_df.PC2)\n",
    "plt.title('PCA Graph')\n",
    "plt.xlabel(['PC1 - {0}%'.format(per_var[0])])\n",
    "plt.ylabel(['PC2 - {0}%'.format(per_var[1])])\n",
    "\n",
    "df1 = pca_df[['PC1', 'PC2']]\n",
    "# plt.savefig('Results/pca_graph.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "- Using the information from the scree plot and the plotted PCA Graph of the two most significant PCAâ€™s, the percentage of variation explained by PC1 and PC2 was 24.1% and 20.6% respectively. \n",
    "- Looking at the data above it is clear that PC1 and PC2 are not sufficient enough to explain most of the variance in the data. \n",
    "- This highlights that dimensionality reduction on this dataset of 5 features is not viable as there is no one dominating PCA. \n",
    "- Also, reducing the number of features is not possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "(1) Decision Tree Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = pd.read_csv('Results/ModelData.csv', low_memory=False)\n",
    "df_sample = df_filtered.sample(100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a sample of 100,000 objects and calculate mahalanobis distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mahalanobis Distance\n",
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np\n",
    "\n",
    "# Find the mean for each attribute\n",
    "df_mahalonobis = df_sample\n",
    "\n",
    "data_mahala = df_mahalonobis.to_numpy().astype(float)\n",
    "mean = np.mean(data_mahala, axis=0)\n",
    "\n",
    "def calculateMahalanobis(x, mean, data):\n",
    "    diff_data_mean = x - mean\n",
    "\n",
    "    data_mahala = np.transpose(data)\n",
    "    covM = np.cov(data_mahala, bias=False)\n",
    "    invCovM = np.linalg.inv(covM)\n",
    "\n",
    "    tem1 = np.dot(diff_data_mean, invCovM)\n",
    "    tem2 = np.dot(tem1, np.transpose(diff_data_mean))\n",
    "\n",
    "    m_distance = np.sqrt(tem2)\n",
    "\n",
    "    return m_distance\n",
    "\n",
    "mahala_distances = []\n",
    "\n",
    "for x in data_mahala:\n",
    "    distance = calculateMahalanobis(x, mean, data_mahala)\n",
    "    mahala_distances.append(round(distance,3))\n",
    "    # print(np.reshape(distance,-1))\n",
    "\n",
    "print(mahala_distances[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the distances to see if there is any patterns. Try to split into 3 distinct classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mahala_distances)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# std = statistics.pstdev(euclidean_distances)\n",
    "# mean_distance = statistics.mean(euclidean_distances)\n",
    "labels = []\n",
    "m = max(mahala_distances)\n",
    "\n",
    "for distance in mahala_distances:\n",
    "    if 0 <= distance <= 1:\n",
    "        labels.append(1)\n",
    "    elif 1 < distance <= 1.5 :\n",
    "        labels.append(2)\n",
    "    else:\n",
    "        labels.append(3)\n",
    "\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mahalonobis\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import statistics\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# the log parameter shows the the data in a semi-log scale to see frequency of groups with smaller counts\n",
    "plt.hist(labels,  edgecolor='black', log=True)\n",
    "\n",
    "median_distance = statistics.mean(labels)\n",
    "color = '#fc4f30'\n",
    "\n",
    "std = statistics.pstdev(labels)\n",
    "\n",
    "# Adds a median line allowing us to see infer information about the data\n",
    "plt.axvline(median_distance, color=color, label=f'Median Distance {round(median_distance,2)}', linewidth=2)\n",
    "plt.axvline(median_distance + std, color='blue', label=f'Standard Deviation {round(median_distance + std,2)}', linewidth=2)\n",
    "plt.axvline(median_distance + 2*std, color='red', label=f'Standard Deviation {round(median_distance + 2*std,2)}', linewidth=2)\n",
    "plt.axvline(median_distance + 3*std, color='green', label=f'Standard Deviation {round(median_distance + 3*std,2)}', linewidth=2)\n",
    "\n",
    "# plt.legend()\n",
    "plt.title('Mahala Distances by ')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Distances')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the distances into 3 class intervals. 0-1 as class one, 1-1.5 as class two and 1.5+ is class 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Decision Tree Classifier, make a 70-30 split. 70% training and 30% test. \n",
    "This split resulted in the highest performance of the model and in turn the accuracy. On\n",
    "average there was an accuracy of 98%. A visualization of the Decision Tree is as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_sample # Features\n",
    "y = pd.DataFrame(labels) # class labels\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3) # 70% training and 30% test\n",
    "\n",
    "# Create Decision Tree classifier\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Train DTC\n",
    "clf = clf.fit(x_train, y_train)\n",
    "\n",
    "# predict the response for the train dataset\n",
    "# y_pred = clf.predict(x_train)\n",
    "# print(f\"Train Accuracy: {metrics.accuracy_score(y_train, y_pred)}\")\n",
    "\n",
    "# predict the response for the test dataset\n",
    "y_pred = clf.predict(x_test)\n",
    "print(f\"Test Accuracy: {metrics.accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from six import StringIO\n",
    "from sklearn import tree\n",
    "from IPython.display import Image  \n",
    "import pydotplus\n",
    "\n",
    "feature_cols = ['averageRating', 'numVotes', 'startYear', 'runtimeMinutes']\n",
    "data = tree.export_graphviz(clf, out_file=None,  \n",
    "                filled=True, rounded=True, feature_names = feature_cols,class_names=['1','2', '3'])\n",
    "\n",
    "graph = pydotplus.graph_from_dot_data(data) \n",
    "graph.write_png('DecisionTree.png')\n",
    "Image(graph.create_png())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
